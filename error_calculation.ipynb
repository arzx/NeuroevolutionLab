{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NeuralNetwork' from 'network' (/opt/homebrew/Caskroom/miniconda/base/envs/rl_exercises/lib/python3.10/site-packages/network.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/arturganzha/NeuroevolutionLab/error_calculation.ipynb Zelle 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/arturganzha/NeuroevolutionLab/error_calculation.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnetwork\u001b[39;00m \u001b[39mimport\u001b[39;00m NeuralNetwork, Dense, linear, relu\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arturganzha/NeuroevolutionLab/error_calculation.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mgym\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arturganzha/NeuroevolutionLab/error_calculation.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'NeuralNetwork' from 'network' (/opt/homebrew/Caskroom/miniconda/base/envs/rl_exercises/lib/python3.10/site-packages/network.py)"
     ]
    }
   ],
   "source": [
    "from network import NeuralNetwork, Dense, linear, relu\n",
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting network\n",
      "  Using cached network-0.1-py3-none-any.whl\n",
      "Installing collected packages: network\n",
      "Successfully installed network-0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.18440509  0.12435395]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "num_actions = env.action_space.n\n",
    "obs_shape = env.observation_space.shape[0]\n",
    "\n",
    "# Das Q-Net erhält als Eingabe einen State und berechnet für alle möglichen\n",
    "# Aktionen einen Q-Wert.\n",
    "q_net = NeuralNetwork([\n",
    "    Dense(obs_shape, 32, activation=relu),\n",
    "    Dense(32, 32, activation=relu),\n",
    "    Dense(32, num_actions, activation=linear),\n",
    "])\n",
    "\n",
    "# Hier beispielhaft an der Transition zwischen initialen State und dem darauf\n",
    "# folgenden State. Üblicherweise wird hier ein Batch aus dem ReplayBuffer\n",
    "# verwendet.\n",
    "state, info = env.reset()\n",
    "action = env.action_space.sample()\n",
    "next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "done = terminated or truncated\n",
    "\n",
    "# Berechne Q-Values für alle möglichen Aktionen\n",
    "q_values = q_net.forward(state)\n",
    "print(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman-Gleichung und Temporal Difference Error\n",
    "Bellman-Gleichung: $Q^\\pi(s, a) = r + \\gamma Q^\\pi(s', \\pi(s'))$ <br><br>\n",
    "Die Policy $\\pi(s) = \\arg\\max_a Q(s, a)$ berechnet für einen gegebenen Zustand die Aktion mit dem höchsten Q-Wert. <br>\n",
    "Folglich ist $Q^\\pi(s', \\pi(s'))$ der Q-Wert genau dieser Aktion bei gegebenen State.\n",
    "\n",
    "Für das Training des neuronalen Netzes ergibt sich hier in der Praxis aber ein Problem. Der _Temporal Difference Errror_ berechnet sich aus dem Q-Wert der ausgeführten Aktion $a$ in State $s$ und der Aktion mit dem höchsten Q-Wert $\\pi(s')$ im Zustand $s'$.\n",
    "Das neuronale Netz berechnet allerdings Q-Werte für alle Aktionen gleichzeitig (aus Effizienzgründen). <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.95433478]\n"
     ]
    }
   ],
   "source": [
    "# Berechne Bellman-Gleichung\n",
    "GAMMA = 0.95\n",
    "\n",
    "# Rechte Seite des Temporal Difference Errors\n",
    "q_target = reward + GAMMA * np.max(q_net.forward(next_state), axis=1) * (1 - done)\n",
    "\n",
    "print(q_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir müssen den berechneten Fehler (_Temporal Difference Error_) also so anpassen, dass wir damit unser Netz trainieren können. <br>\n",
    "\n",
    "Temporal Difference Error: $\\delta = Q^\\pi(s, a) = r + \\gamma Q^\\pi(s', \\pi(s'))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.95433478 0.12435395]]\n"
     ]
    }
   ],
   "source": [
    "# Beispielhafte Lösung.\n",
    "q_target = q_net.forward(state)\n",
    "q_target[0, action] = reward + GAMMA * np.max(q_net.forward(next_state), axis=1)\n",
    "\n",
    "print(q_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.13873986,  0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values - q_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Außerdem ist es für das Training erforderlich, dass wir eine Fehlerfunktion verwenden, bei der wir einem Gradienten folgen können. Hier können wir z.&nbsp;B. den berechnten Temporal Difference Error einfach quadrieren (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.57420821, 0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q_values - q_target)**2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
