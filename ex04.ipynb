{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuroevolution: Exercise 4\n",
    "=========\n",
    "###### Artur Ganzha 10019651\n",
    "---------\t\n",
    "###### Raul Gorek 10061333\n",
    "---------\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_bcel(prediction, ground_truth):\n",
    "    x =  np.where(ground_truth == 0, 1.0 / (1.0 - prediction), -1.0 / prediction)\n",
    "    return x\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.W = np.random.uniform(-1, 1,(self.input_size,self.output_size))\n",
    "        self.B = np.zeros((1, self.output_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.fw = x\n",
    "        return np.dot(x, self.W) + self.B\n",
    "    \n",
    "    def backward(self, d, lr):\n",
    "        d_w = np.dot(self.fw.T, d)\n",
    "        d_e = np.dot(d, self.W.T)\n",
    "        d_b = np.sum(d, axis=0, keepdims=True)\n",
    "        self.W -= lr * d_w / self.fw.shape[0]\n",
    "        self.B -= lr * d_b / self.fw.shape[0]\n",
    "        return d_e\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.fw = x\n",
    "        return x * (x > 0)\n",
    "    \n",
    "    def backward(self, d, lr):\n",
    "        return d * np.where(self.fw > 0, 1.0, 0.0)\n",
    "    \n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.fw = x\n",
    "        self.out = 1.0 / (1.0 + np.exp(-x))\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, d, lr):\n",
    "        return d * (self.out * (1.0 - self.out))\n",
    "    \n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward_pass(self, deriv, lr):\n",
    "        for layer in reversed(self.layers):\n",
    "            deriv = layer.backward(deriv, lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights(self):\n",
    "    for layer in self.layers:\n",
    "        if type(layer) == Linear:\n",
    "            print(\"Bias: \\n\", layer.B)\n",
    "            print(\"Weights: \\n\", layer.W)\n",
    "\n",
    "def mutate_weights(self):\n",
    "    for layer in self.layers:\n",
    "        if type(layer) == Linear:\n",
    "            layer.W += np.random.normal(0,1, size=layer.W.shape)\n",
    "            layer.B += np.random.normal(0,1,size=layer.B.shape)\n",
    "\n",
    "\n",
    "NeuralNetwork.mutate_weights = mutate_weights\n",
    "NeuralNetwork.print_weights = print_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias: \n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Weights: \n",
      " [[ 0.72609461 -0.91190971  0.56544948 -0.40530036  0.15721535 -0.1135427\n",
      "  -0.42260325 -0.3547749 ]\n",
      " [ 0.47263947 -0.13143757  0.83154339  0.36383336  0.93500126 -0.43025643\n",
      "  -0.23235111  0.29828577]]\n",
      "Bias: \n",
      " [[0.]]\n",
      "Weights: \n",
      " [[ 0.20748512]\n",
      " [ 0.63852403]\n",
      " [ 0.29486175]\n",
      " [-0.31485759]\n",
      " [ 0.47829978]\n",
      " [ 0.46640971]\n",
      " [ 0.47934609]\n",
      " [ 0.04988258]]\n",
      "Bias: \n",
      " [[-0.84037203 -1.06723698  0.48104754  0.80929495 -1.61986695 -0.85353387\n",
      "  -0.72197012  0.84513447]]\n",
      "Weights: \n",
      " [[-0.80476021 -1.147357   -0.11624117  0.21554518  0.21553871 -1.91694353\n",
      "   0.27276284 -0.97173543]\n",
      " [ 0.11188649  0.99873087  1.0888878   1.61811882  0.2144705  -0.40618126\n",
      "   0.5764016  -0.58597989]]\n",
      "Bias: \n",
      " [[-0.96813439]]\n",
      "Weights: \n",
      " [[-0.42506515]\n",
      " [-0.76624234]\n",
      " [-0.61190791]\n",
      " [-0.65981488]\n",
      " [ 0.37405602]\n",
      " [ 1.26893754]\n",
      " [-2.80023014]\n",
      " [ 0.81757682]]\n"
     ]
    }
   ],
   "source": [
    "arch = [\n",
    "    Linear(2,8),\n",
    "    ReLU(),\n",
    "    Linear(8,1),\n",
    "    Sigmoid()\n",
    "]\n",
    "net = NeuralNetwork(arch)\n",
    "net.print_weights()\n",
    "net.mutate_weights()\n",
    "net.print_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "labels = np.array([[0.0], [1.0], [1.0], [0.0]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(net: NeuralNetwork):\n",
    "    y_hat = net.forward_pass(batch)\n",
    "    accuracy = np.mean(1 - np.abs(labels - y_hat))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elitist_selection(individuals: list[NeuralNetwork], n):\n",
    "    f = np.zeros(shape=(len(individuals,)))\n",
    "    for i, net in enumerate(individuals):\n",
    "        f[i]= (fitness(net))\n",
    "    return [individuals[i] for i in (-f).argsort()[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_proportional_selection(individuals: list[NeuralNetwork], n):\n",
    "    f = np.zeros(shape=(len(individuals,)))\n",
    "    for i, net in enumerate(individuals):\n",
    "        f[i]= (fitness(net))\n",
    "    return list(np.random.choice(individuals, size=(n,), p=f/f.sum(), replace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test mit Elitist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of fittest:  0.9999999995066218\n",
      "XOR of fittest: \n",
      " [[1.90903964e-09]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [6.44732017e-11]]\n",
      "Weights of fittest:\n",
      "Bias: \n",
      " [[ -2.93865288 -12.40487243   0.2822418    7.88665885   2.31474114\n",
      "    6.73714254  -1.63847066   3.34725324]]\n",
      "Weights: \n",
      " [[  7.09256619  -5.76258805   0.3749416   -6.23455898  -5.04613591\n",
      "    4.6795403   13.35063804   5.02091457]\n",
      " [ -2.36123224  -4.61007846  -4.71935196   1.1495039    0.0906418\n",
      "   -7.31834872 -11.83062061  -4.79061021]]\n",
      "Bias: \n",
      " [[6.46558763]]\n",
      "Weights: \n",
      " [[  8.35737261]\n",
      " [ -6.2060298 ]\n",
      " [  3.56155371]\n",
      " [  8.32266465]\n",
      " [ -5.08906826]\n",
      " [ -6.05262743]\n",
      " [ 15.24663151]\n",
      " [-12.13777297]]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "popsize = 1\n",
    "elite = 1\n",
    "individuals = [NeuralNetwork([Linear(2,8), ReLU(), Linear(8,1), Sigmoid()]) for _ in range(popsize)]\n",
    "\n",
    "max_compute_time = 5.0\n",
    "t1 = time.perf_counter()\n",
    "it = 0\n",
    "while time.perf_counter() - t1 < max_compute_time:\n",
    "    individuals = elitist_selection(individuals, elite)\n",
    "    cop = copy.deepcopy(individuals[0])\n",
    "    cop.mutate_weights()\n",
    "    individuals.append(cop)\n",
    "    it += 1\n",
    "\n",
    "fittest = sorted(individuals, key=lambda x: fitness(x))[0]\n",
    "print(\"Accuracy of fittest: \", fitness(fittest))\n",
    "print(\"XOR of fittest: \\n\", fittest.forward_pass([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]))\n",
    "print(\"Weights of fittest:\")\n",
    "print(fittest.print_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test mit Fitness Proportional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37869/325054922.py:43: RuntimeWarning: overflow encountered in exp\n",
      "  self.out = 1.0 / (1.0 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of fittest:  0.5052999723313124\n",
      "XOR of fittest: \n",
      " [[9.78800111e-001]\n",
      " [1.70194592e-195]\n",
      " [1.00000000e+000]\n",
      " [4.93235036e-044]]\n",
      "Weights of fittest:\n",
      "Bias: \n",
      " [[ -6.86884305   2.98348224  -5.23497388 -11.64712371 -22.55188824\n",
      "    4.3329667    2.3208159  -11.00170399]]\n",
      "Weights: \n",
      " [[ -9.02863546   1.43989013  15.50261694  -3.38781866 -17.32870493\n",
      "   13.03304171 -16.36512616 -24.66635871]\n",
      " [ -8.47247252   1.94502589 -12.11061017 -24.74442206 -25.33241005\n",
      "   -0.66008609  16.95587228   1.53946307]]\n",
      "Bias: \n",
      " [[9.37712965]]\n",
      "Weights: \n",
      " [[ -5.93493489]\n",
      " [  1.35415428]\n",
      " [-29.3129626 ]\n",
      " [ -1.91400149]\n",
      " [  3.07503314]\n",
      " [ -9.37834061]\n",
      " [ 13.37940857]\n",
      " [-17.64581053]]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "popsize = 40\n",
    "elite = 5\n",
    "individuals = [NeuralNetwork([Linear(2,8), ReLU(), Linear(8,1), Sigmoid()]) for _ in range(popsize)]\n",
    "\n",
    "max_compute_time = 5.0\n",
    "t1 = time.perf_counter()\n",
    "while time.perf_counter() - t1 < max_compute_time:\n",
    "    individuals = fitness_proportional_selection(individuals, elite)\n",
    "    mutated_ones = []\n",
    "    i = 0\n",
    "    while len(mutated_ones) < popsize - elite:\n",
    "        cop = copy.deepcopy(individuals[i])\n",
    "        cop.mutate_weights()\n",
    "        mutated_ones.append(cop)\n",
    "        i += 1\n",
    "        i %= elite\n",
    "    individuals += mutated_ones\n",
    "\n",
    "fittest = sorted(individuals, key=lambda x: fitness(x))[0]\n",
    "print(\"Accuracy of fittest: \", fitness(fittest))\n",
    "print(\"XOR of fittest: \\n\", fittest.forward_pass([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]))\n",
    "print(\"Weights of fittest:\")\n",
    "print(fittest.print_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beobachtungen\n",
    "Elitist Selection konvergiert manchmal gut, manchmal bleibt er in lokalem Minimum stecken.\n",
    "Fitness Prop Selection haben wir gar nicht zur Konvergenz gebracht.  \n",
    "- Liegt warhscheinlich daran, dass die Fitnesswerte oft viel zu nah beieinander liegen (Durch Sigmoid am Ende). Dadurch werden viele Netze mit schlechten Gewichten genommen und es gibt gar keine richtige Chance zu konvergieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergleich der beiden Modelle (Evo vs. Backprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vergleich Genauigkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xor_evo(max_compute_time, popsize, elite):\n",
    "    individuals = [NeuralNetwork([Linear(2,8), ReLU(), Linear(8,1), Sigmoid()]) for _ in range(popsize)]\n",
    "    t1 = time.perf_counter()\n",
    "    while time.perf_counter() - t1 < max_compute_time:\n",
    "        individuals = elitist_selection(individuals, elite)\n",
    "        mutated_ones = [copy.deepcopy(individuals[0])]\n",
    "        mutated_ones[0].mutate_weights()\n",
    "        i = 1\n",
    "        while len(mutated_ones) < popsize - elite:\n",
    "            cop = copy.deepcopy(individuals[i])\n",
    "            cop.mutate_weights()\n",
    "            mutated_ones.append(cop)\n",
    "            i += 1\n",
    "            i %= elite\n",
    "        individuals += mutated_ones\n",
    "\n",
    "    fittest = sorted(individuals, key=lambda x: fitness(x))[0]\n",
    "    return fitness(fittest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_batch = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "xor_label = np.array([[0.0], [1.0], [1.0], [0.0]])\n",
    "def train_backprop(max_time, batch_size, lr):\n",
    "    net = NeuralNetwork([\n",
    "        Linear(2,8),\n",
    "        ReLU(),\n",
    "        Linear(8,1),\n",
    "        Sigmoid()\n",
    "    ])\n",
    "    t1 = time.perf_counter()\n",
    "    while time.perf_counter() - t1 < max_time:\n",
    "        batch = np.random.randint(0,2, size=(batch_size, 2))\n",
    "        labels = np.logical_xor(batch[:, 0], batch[:, 1]).astype(float)[:,None]\n",
    "        prediction = net.forward_pass(batch)\n",
    "        loss_deriv = derivative_bcel(prediction, labels)\n",
    "        net.backward_pass(loss_deriv, lr)\n",
    "    prediction = net.forward_pass(xor_batch)\n",
    "    return np.mean(1 - np.abs(xor_label - prediction))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evo Acc:  87.49999999966167 %\n",
      "BP Acc:  99.74897834961276 %\n",
      "Evo is  -12.279803314896453 % besser als Backpropagation\n",
      "Diff in %:  12.24897834995109\n"
     ]
    }
   ],
   "source": [
    "train_time = 5.0\n",
    "num_exp = 10\n",
    "mean_evo_acc = np.zeros((10,))\n",
    "mean_bp_acc = np.zeros_like(mean_evo_acc)\n",
    "for i in range(num_exp):\n",
    "    mean_evo_acc[i] = train_xor_evo(train_time, popsize=1, elite=1)\n",
    "    mean_bp_acc[i] = train_backprop(train_time, batch_size=64, lr=0.05)\n",
    "mean_evo_acc = np.mean(mean_evo_acc)\n",
    "mean_bp_acc = np.mean(mean_bp_acc)\n",
    "print(\"Evo Acc: \", 100*mean_evo_acc, \"%\")\n",
    "print(\"BP Acc: \", 100*mean_bp_acc, \"%\")\n",
    "print(\"Evo is \", 100*((mean_evo_acc/mean_bp_acc)-1), \"% besser als Backpropagation\")\n",
    "print(\"Diff in %: \", 100 * np.abs(mean_evo_acc - mean_bp_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vergleich Der Trainingszeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xor_evo_time(acc, popsize, elite):\n",
    "    individuals = [NeuralNetwork([Linear(2,8), ReLU(), Linear(8,1), Sigmoid()]) for _ in range(popsize)]\n",
    "    max_acc = 0.0\n",
    "    t1 = time.perf_counter()\n",
    "    while max_acc < acc:\n",
    "        individuals = elitist_selection(individuals, elite)\n",
    "        max_acc = fitness(individuals[0])\n",
    "        mutated_ones = []\n",
    "        i = 0\n",
    "        while len(mutated_ones) < popsize - elite:\n",
    "            cop = copy.deepcopy(individuals[i])\n",
    "            cop.mutate_weights()\n",
    "            mutated_ones.append(cop)\n",
    "            i += 1\n",
    "            i %= elite\n",
    "        individuals += mutated_ones\n",
    "        individuals += mutated_ones\n",
    "    return time.perf_counter() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_batch = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "xor_label = np.array([[0.0], [1.0], [1.0], [0.0]])\n",
    "def train_backprop_time(acc, batch_size, lr):\n",
    "    net = NeuralNetwork([\n",
    "        Linear(2,8),\n",
    "        ReLU(),\n",
    "        Linear(8,1),\n",
    "        Sigmoid()\n",
    "    ])\n",
    "    max_acc = 0.0\n",
    "    t1 = time.perf_counter()\n",
    "    while max_acc < acc:\n",
    "        batch = np.random.randint(0,2, size=(batch_size, 2))\n",
    "        labels = np.logical_xor(batch[:, 0], batch[:, 1]).astype(float)[:,None]\n",
    "        prediction = net.forward_pass(batch)\n",
    "        loss_deriv = derivative_bcel(prediction, labels)\n",
    "        net.backward_pass(loss_deriv, lr)\n",
    "        max_acc = np.mean(1 - np.abs(xor_label - net.forward_pass(xor_batch)))\n",
    "    return time.perf_counter() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evo\n",
      "BP\n",
      "Evo\n",
      "BP\n",
      "Evo\n",
      "BP\n",
      "Evo\n",
      "BP\n",
      "Evo\n",
      "BP\n",
      "Evo Durchschnittszeit:  0.014743297200038797 s\n",
      "BP Durchschnittszeit:  0.14763163959978556 s\n",
      "Evo is  90.01345697981382 % schneller als Backpropagation\n",
      "Diff in s:  0.13288834239974676\n"
     ]
    }
   ],
   "source": [
    "min_acc = 0.7\n",
    "num_exp = 5\n",
    "mean_evo_time = np.zeros((10,))\n",
    "mean_bp_time = np.zeros_like(mean_evo_time)\n",
    "for i in range(num_exp):\n",
    "    mean_evo_time[i] = train_xor_evo_time(acc=min_acc, popsize=10, elite=3)\n",
    "    print(\"Evo\")\n",
    "    mean_bp_time[i] = train_backprop_time(acc=min_acc, batch_size=64, lr=0.05)\n",
    "    print(\"BP\")\n",
    "mean_evo_time = np.mean(mean_evo_time)\n",
    "mean_bp_time = np.mean(mean_bp_time)\n",
    "print(\"Evo Durchschnittszeit: \", mean_evo_time, \"s\")\n",
    "print(\"BP Durchschnittszeit: \", mean_bp_time, \"s\")\n",
    "print(\"Evo is \", 100*(1-(mean_evo_time/mean_bp_time)), \"% schneller als Backpropagation\")\n",
    "print(\"Diff in s: \", np.abs(mean_evo_time - mean_bp_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So würden wir die Laufzeiten vergleichen. Funktioniert auch für geringere Acc's. Für höhere bleibt er einfach zu oft in lokalen Minimas hängen xD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
