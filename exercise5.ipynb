{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuroevolution: Exercise 4\n",
    "=========\n",
    "###### Artur Ganzha 10019651\n",
    "---------\t\n",
    "###### Raul Gorek 10061333\n",
    "---------\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random as random\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "num_actions = env.action_space.n\n",
    "obs_shape = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Nutzen Sie den Code Ihres neuronalen Netzes, um ein Q-Netzwerk zu implementieren, welches als Eingabe die aktuelle Wagenposition, die Beschleunigung des Wagens, den Winkel des Stabes sowie die Beschleunigung des Stabwinkels erh ̈alt, um daraus die Q-Values zu berechnen. Es wird empfohlen, sich dabei an der Architektur, aus dem Notebook error_calculation.ipynb, zu orientieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NeuralNet\n",
    "def derivative_bcel(prediction, ground_truth):\n",
    "    x =  np.where(ground_truth == 0, 1.0 / (1.0 - prediction), -1.0 / prediction)\n",
    "    return x\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.W = np.random.uniform(-1, 1,(self.input_size,self.output_size))\n",
    "        self.B = np.zeros((1, self.output_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.fw = x\n",
    "        return np.dot(x, self.W) + self.B\n",
    "    \n",
    "    def backward(self, d, lr):\n",
    "        d_w = np.dot(self.fw.T, d)\n",
    "        d_e = np.dot(d, self.W.T)\n",
    "        d_b = np.sum(d, axis=0, keepdims=True)\n",
    "        self.W -= lr * d_w / self.fw.shape[0]\n",
    "        self.B -= lr * d_b / self.fw.shape[0]\n",
    "        return d_e\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.fw = x\n",
    "        return x * (x > 0)\n",
    "    \n",
    "    def backward(self, d, lr):\n",
    "        return d * np.where(self.fw > 0, 1.0, 0.0)\n",
    "    \n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.fw = x\n",
    "        self.out = 1.0 / (1.0 + np.exp(-x))\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, d, lr):\n",
    "        return d * (self.out * (1.0 - self.out))\n",
    "    \n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward_pass(self, deriv, lr):\n",
    "        for layer in reversed(self.layers):\n",
    "            deriv = layer.backward(deriv, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([\n",
    "    Linear(obs_shape,32), ReLU(),\n",
    "    Linear(32,32), ReLU(),\n",
    "    Linear(32, num_actions),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### q_values\n",
      "[[-0.09657385  0.20543849]]\n",
      "#### next state\n",
      "[-0.01413328  0.17400022 -0.03787411 -0.3036097 ]\n",
      "#### action\n",
      "1\n",
      "#### reward\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "state, info = env.reset()\n",
    "action = env.action_space.sample()\n",
    "next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "done = terminated or truncated\n",
    "\n",
    "# Berechne Q-Values für alle möglichen Aktionen\n",
    "q_values = nn.forward_pass(state)\n",
    "print(\"#### q_values\")\n",
    "print(q_values)\n",
    "print(\"#### next state\")\n",
    "print(next_state)\n",
    "print(\"#### action\")\n",
    "print(action)\n",
    "print(\"#### reward\")\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\",(\"state\", \"action\", \"next_state\", \"reward\", \"done\"))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31712957]\n"
     ]
    }
   ],
   "source": [
    "# Berechne Bellman-Gleichung\n",
    "GAMMA = 0.95\n",
    "\n",
    "# Rechte Seite des Temporal Difference Errors\n",
    "q_target = reward + GAMMA * np.max(nn.forward_pass(next_state), axis=1) * (1 - done)\n",
    "\n",
    "print(q_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abschließend soll der Agent implementiert werden. Fu ̈r das Training des neuronalen Netzes soll die Bellman-Gleichung (Q(s, a) = r + γQ(s′, π(s′))) verwendet werden. Verwenden Sie den Mean Squared Error Loss um den Fehler zu bestimmen. U ̈berlegen Sie sich eine M ̈oglichkeit, wie sie einen guten Tradeoff zwischen Exploration und Exploitation implementieren k ̈onnen. Stellen Sie den Trainingsverlauf in einer Grafik dar. An der y-Achse soll dabei der akkumulierte Reward am Ende einer Episode abzulesen sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = (q_values - q_target) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "BATCH_SIZE = 64\n",
    "MEMORY_CAPACITY = 10000\n",
    "EPSILON_DECAY = 0.995\n",
    "MIN_EPSILON = 0.01\n",
    "GAMMA = 0.95\n",
    "TARGET_UPDATE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(q_values, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(range(num_actions))\n",
    "    else:\n",
    "        return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Neural Network and Memory\n",
    "nn = NeuralNetwork([\n",
    "    Linear(obs_shape, 32), ReLU(),\n",
    "    Linear(32, 32), ReLU(),\n",
    "    Linear(32, num_actions),\n",
    "])\n",
    "\n",
    "target_nn = NeuralNetwork([\n",
    "    Linear(obs_shape, 32), ReLU(),\n",
    "    Linear(32, 32), ReLU(),\n",
    "    Linear(32, num_actions),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
