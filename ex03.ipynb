{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuroevolution: Exercise 1 \n",
    "=========\n",
    "###### Artur Ganzha 10019651\n",
    "---------\t\n",
    "###### Raul Gorek 10061333\n",
    "---------\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "y = np.array([0.0, 1.0, 1.0, 0.0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor(a, b):\n",
    "    return (a or b) and not (a and b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufgabe 2\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers = layers\n",
    "        for layer in self.layers:\n",
    "            layer.init()\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "class Linear:\n",
    "    def __init__(self, input_size, output_size, activation = relu):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "    \n",
    "    def init(self):\n",
    "        self.W = np.random.uniform(-1, 1,(self.input_size,self.output_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.in_pass = x\n",
    "        self.fw = np.dot(self.in_pass, self.W)\n",
    "        self.act_forward = self.activation(self.fw)\n",
    "        return self.act_forward\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Linear(2,8),\n",
    "    Linear(8, 4),\n",
    "    Linear(4,1, sigmoid)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       ],\n",
       "       [0.17382848]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = np.array([[0,0],[0,1]])\n",
    "neural_net.forward_pass(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufgabe 3\n",
    "def binary_cross_entropy_loss(prediction, ground_truth):\n",
    "    return -(ground_truth * np.log(prediction) + (1-ground_truth) * np.log(1-prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_bcel(prediction, ground_truth):\n",
    "    x =  np.where(ground_truth == 0, 1.0 / (1.0 - prediction), -1.0 / prediction)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_relu(z):\n",
    "    return z > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_sigmoid(z):\n",
    "    x = sigmoid(z)\n",
    "    return x * (1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(self, deriv_loss, learn_rate, prediction, ground_truth):\n",
    "    deriv = deriv_loss(prediction, ground_truth)\n",
    "    for layer in reversed(self.layers):\n",
    "        if layer.activation == relu:\n",
    "            deriv_act = derivative_relu(layer.fw)\n",
    "        elif layer.activation == sigmoid:\n",
    "            deriv_act = derivative_sigmoid(layer.fw)\n",
    "        print(deriv_act.shape, deriv.shape)\n",
    "        deriv = deriv * deriv_act\n",
    "        print(deriv.shape)\n",
    "        print(\"--\", layer.in_pass.shape)\n",
    "        update_deriv = np.dot(layer.in_pass.T, deriv)\n",
    "        print(update_deriv.shape)\n",
    "        layer.W -= learn_rate * update_deriv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralNetwork.backward_pass = backward_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1) (4, 1)\n",
      "(4, 1)\n",
      "-- (4, 4)\n",
      "(4, 1)\n",
      "(4, 4) (4, 1)\n",
      "(4, 4)\n",
      "-- (4, 8)\n",
      "(8, 4)\n",
      "(4, 8) (4, 4)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,4) (4,8) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/arturganzha/NeuroevolutionLab/ex03.ipynb Zelle 17\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arturganzha/NeuroevolutionLab/ex03.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     y_hat \u001b[39m=\u001b[39m neural_net\u001b[39m.\u001b[39mforward_pass(batch)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arturganzha/NeuroevolutionLab/ex03.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     loss \u001b[39m=\u001b[39m binary_cross_entropy_loss(y_hat, labels)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/arturganzha/NeuroevolutionLab/ex03.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     neural_net\u001b[39m.\u001b[39;49mbackward_pass(derivative_bcel, \u001b[39m0.01\u001b[39;49m, y_hat, labels)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arturganzha/NeuroevolutionLab/ex03.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(loss)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arturganzha/NeuroevolutionLab/ex03.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(neural_net\u001b[39m.\u001b[39mforward_pass(batch))\n",
      "\u001b[1;32m/Users/arturganzha/NeuroevolutionLab/ex03.ipynb Zelle 17\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arturganzha/NeuroevolutionLab/ex03.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     deriv_act \u001b[39m=\u001b[39m derivative_sigmoid(layer\u001b[39m.\u001b[39mfw)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arturganzha/NeuroevolutionLab/ex03.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(deriv_act\u001b[39m.\u001b[39mshape, deriv\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/arturganzha/NeuroevolutionLab/ex03.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m deriv \u001b[39m=\u001b[39m deriv \u001b[39m*\u001b[39;49m deriv_act\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arturganzha/NeuroevolutionLab/ex03.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(deriv\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arturganzha/NeuroevolutionLab/ex03.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m--\u001b[39m\u001b[39m\"\u001b[39m, layer\u001b[39m.\u001b[39min_pass\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,4) (4,8) "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch = np.array([[0,0],[0,1], [1,0], [1,1]])\n",
    "labels = np.array([[0.0], [1.0], [1.0], [0.0]]) \n",
    "for i in range(num_epochs):\n",
    "    y_hat = neural_net.forward_pass(batch)\n",
    "    loss = binary_cross_entropy_loss(y_hat, labels)\n",
    "    neural_net.backward_pass(derivative_bcel, 0.01, y_hat, labels)\n",
    "    print(loss)\n",
    "print(neural_net.forward_pass(batch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
